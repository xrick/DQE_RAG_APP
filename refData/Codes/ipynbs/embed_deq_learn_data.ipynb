{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4d56b7-73f2-4008-aabb-3188ce5db7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83a4f2e-d146-4bcd-8d77-064436ac0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_selected = df[['模块', '严重度', '问题现象描述', '原因分析', '改善对策', '经验萃取', '评审后优化', '评分']]\n",
    "# df_selected.columns = ['module', 'severity', 'question', 'cause', 'improvement', 'experience', 'judge', 'score']\n",
    "# loader = CSVLoader(file_path=\"../../../doc/dsq_learn_sheet_eng_titles.csv\")\n",
    "# data = loader.load()\n",
    "# data_len = len(data)\n",
    "# print(data_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aed3b1-817c-4e79-9fc6-ccd52a8d3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53680f0-41ab-45e4-bbe0-ac6444a6c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embedding_DQE(csv_file=None,qKey=\"Question\", qAns=\"Answer\"):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     # 合併重複問題的答案\n",
    "#     df_clean = df.groupby(qKey)[qAns].apply(lambda x: '\\n'.join(x.unique())).reset_index()\n",
    "#     # 正規化代碼格式\n",
    "#     \"\"\"\n",
    "#     這是一個使用正則表達式的字串替換操作。目的是將Answer欄位中的參數設定格式標準化。\n",
    "#     例如，將類似\"PcdCfgPeApmEnable = 0\"中的空格去除，變成\"PcdCfgPeApmEnable=0\"。\n",
    "#     這樣做的好處是統一格式，避免後續處理時因格式不一致導致問題，比如在檢索或解析時無法正確識別參數和數值。\n",
    "#     \"\"\"\n",
    "#     df_clean[qAns] = df_clean[qAns].str.replace(r'(\\w+)\\s*=\\s*(\\d+)', r'\\1=\\2', regex=True)\n",
    "#     # 提取文本數據\n",
    "#     questions = df_clean[qKey].str.strip().tolist()\n",
    "#     answers = df_clean[qAns].str.strip().tolist()\n",
    "#     model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#     question_embeddings = model.encode(\n",
    "#         questions,\n",
    "#         convert_to_tensor=False,\n",
    "#         show_progress_bar=True\n",
    "#     )\n",
    "#     print(question_embeddings.shape)\n",
    "#     dimension = question_embeddings.shape[1]#(#__1)  # 向量維度(通常為384/768)\n",
    "#     quantizer = faiss.IndexFlatL2(dimension)\n",
    "#     index = faiss.IndexIVFFlat(quantizer, dimension, 100)\n",
    "#     index.train(question_embeddings)\n",
    "#     index.add(question_embeddings)\n",
    "#     faiss.write_index(index, \"qa_index_.faiss\")\n",
    "#     #integrated with langchain\n",
    "#     # 創建元數據結構\n",
    "#     metadatas = [{\n",
    "#         \"answer\": ans, \n",
    "#         \"source\": \"內部技術資料庫\",\n",
    "#         \"last_updated\": \"2025-02\"\n",
    "#     } for ans in answers]\n",
    "#     # 建立可持久化向量庫\n",
    "#     vector_db = FAISS.from_embeddings(\n",
    "#         text_embeddings=list(zip(questions, question_embeddings)),\n",
    "#         embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"),\n",
    "#         metadatas=metadatas\n",
    "#     )\n",
    "#     # 保存完整向量庫\n",
    "#     vector_db.save_local(\"tech_support_faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22844541-dd7d-4e90-a24c-2a8d25eecdf1",
   "metadata": {},
   "source": [
    "### Start to Embed the DEQ Learn 28 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16381b39-f953-434b-a29a-3f820df29173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_selected = df[['模块', '严重度', '问题现象描述', '原因分析', '改善对策', '经验萃取', '评审后优化', '评分']]\n",
    "# df_selected.columns = ['module', 'severity', 'question', 'cause', 'improvement', 'experience', 'judge', 'score']\n",
    "# df = pd.read_csv('../../../doc/deq_learn_refine2_correct.csv');\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57cbea4-2e58-4d92-9ee8-c06c7db2f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_question_src = \"questionsource\"\n",
    "key_question = \"question\"\n",
    "key_improve = \"improve\"\n",
    "key_severity = \"severity\"\n",
    "key_module = \"module\"\n",
    "key_cause = \"cause\"\n",
    "key_experience=\"experience\"\n",
    "key_judge = \"judge\"\n",
    "key_score = \"score\"\n",
    "key_shortquestion = \"shortquestion\"\n",
    "key_shortanswer = \"shortanswer\"\n",
    "title_mapping = {\n",
    "    key_question_src:'问题来源',\n",
    "    key_module:'模块',\n",
    "    key_severity:'严重度',\n",
    "    key_question:'问题现象描述',\n",
    "    key_cause:'原因分析',\n",
    "    key_improve:'改善对策',\n",
    "    key_experience:'经验萃取',\n",
    "    key_judge:'评审后优化',\n",
    "    key_score:'评分',\n",
    "}\n",
    "deq_df = pd.read_csv(\"../../../doc/DQE_Lesson_learn_AI_0311_five_refine_question - AI_Trial Run.csv\");\n",
    "questions_src = deq_df[key_question_src].str.strip().tolist();\n",
    "questions = deq_df[key_question].str.strip().tolist();\n",
    "severity = deq_df[key_severity].str.strip().tolist();\n",
    "modules = deq_df[key_module].str.strip().tolist();\n",
    "causes = deq_df[key_cause].str.strip().tolist();\n",
    "improves = deq_df[key_improve].str.strip().tolist();\n",
    "experiences = deq_df[key_experience].str.strip().tolist();\n",
    "judges = deq_df[key_judge].str.strip().tolist();\n",
    "scores = deq_df[key_score].tolist(); #this is int\n",
    "shortquestions = deq_df[key_shortquestion].str.strip().tolist();\n",
    "shortanswers= deq_df[key_shortanswer].str.strip().tolist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ec165b-6b7c-40b5-ae2f-4e9286c2b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_index_for_multicolumns(encode_model:str=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    \n",
    "    if len(questions)!= len(scores) or len(questions)!= len(severity) \\\n",
    "        or len(questions)!= len(modules) or len(questions)!= len(causes):\n",
    "        raise ValueError(\"To embedded data is not equal.\")\n",
    "    model = SentenceTransformer(encode_model);\n",
    "    # embedding two columns: questions_src and  module\n",
    "    question_src_embeddings = model.encode(\n",
    "        questions_src,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    module_embeddings = model.encode(\n",
    "        modules,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(f\"the encoded question shape is {question_src_embeddings.shape}\");\n",
    "    combined_embeddings = np.concatenate([question_src_embeddings, module_embeddings], axis=1)\n",
    "    dimension = combined_embeddings.shape[1]\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    # some the same issues:\n",
    "    # https://github.com/facebookresearch/faiss/issues/1637\n",
    "    print(f\"len of combined_embeddings is {len(combined_embeddings)}\")\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, len(combined_embeddings))\n",
    "    index.train(combined_embeddings)\n",
    "    index.add(combined_embeddings)\n",
    "    faiss.write_index(index, \"../../../web/db/20250317/faiss_index_qsrc_mo.faiss\")\n",
    "    metadatas = [];\n",
    "    for idx in range(len(combined_embeddings)):\n",
    "        tmpdict = {\n",
    "                \"question\": questions[idx],\n",
    "                \"severity\": severity[idx],\n",
    "                \"module\": modules[idx],\n",
    "                \"cause\":causes[idx],\n",
    "                \"improve\":improves[idx],\n",
    "                \"experience\":experiences[idx],\n",
    "                \"judge\":judges[idx],\n",
    "                \"score\":scores[idx],\n",
    "        }\n",
    "        metadatas.append(tmpdict)\n",
    "\n",
    "    # print(metadatas);\n",
    "    # 建立可持久化向量庫\n",
    "    combined_cols = np.concatenate([questions_src, modules], axis=0)\n",
    "    print(f\"combined_cols are:{combined_cols}\")\n",
    "    vector_db = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(combined_cols, combined_embeddings)),\n",
    "        embedding=HuggingFaceEmbeddings(model_name=encode_model),\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    # 保存完整向量庫\n",
    "    vector_db.save_local(\"../../../web/db/20250317/faiss_vdb_qsrc_mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "befea2ba-1584-4a38-b247-e285b67b7639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a692efa323403682d646ac2c486a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8463f8eb9d4da6927b326c50d4fbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the encoded question shape is (5, 384)\n",
      "len of combined_embeddings is 5\n",
      "combined_cols are:['EVT' 'PVT' '首次量产' '量产' 'EVT' '结构' 'ESD' '工艺' '工艺' '软件']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 5 points to 5 centroids: please provide at least 195 training points\n"
     ]
    }
   ],
   "source": [
    "embed_index_for_multicolumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c2cefd-9765-448f-8955-bff57ef2630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_index_for_singlecolumns(encode_model:str=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    \n",
    "    if len(questions)!= len(scores) or len(questions)!= len(severity) \\\n",
    "        or len(questions)!= len(modules) or len(questions)!= len(causes):\n",
    "        raise ValueError(\"To embedded data is not equal.\")\n",
    "    model = SentenceTransformer(encode_model);\n",
    "    # embedding two columns: questions_src and  module\n",
    "    module_embeddings = model.encode(\n",
    "        modules,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(f\"the encoded modules shape is {module_embeddings.shape}\");\n",
    "    \n",
    "    dimension = module_embeddings.shape[1]\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    # some the same issues:\n",
    "    # https://github.com/facebookresearch/faiss/issues/1637\n",
    "    print(f\"len of module_embeddings is {len(module_embeddings)}\")\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, len(module_embeddings))\n",
    "    index.train(module_embeddings)\n",
    "    index.add(module_embeddings)\n",
    "    faiss.write_index(index, \"../../../web/db/20250317/faiss_index_module.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "342915bf-8ee4-4f08-9078-6881269a40cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc61827733aa4bbab4cdf2e7a20cd527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the encoded modules shape is (5, 384)\n",
      "len of module_embeddings is 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 5 points to 5 centroids: please provide at least 195 training points\n"
     ]
    }
   ],
   "source": [
    "embed_index_for_singlecolumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd0974-7083-4376-a4f9-ea686a99434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218e071-6c77-457a-8a25-79200b8e8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_DEQ_Learn_from_csv(csv_file:str=None, paras:dict=None, encode_model:str=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    df = pd.read_csv(csv_file);\n",
    "    # df_clean = df.groupby(question_key)[improve_key].apply(lambda x: '\\n'.join(x.unique())).reset_index()\n",
    "    questions = df[key_question].str.strip().tolist();\n",
    "    severity = df[key_severity].str.strip().tolist();\n",
    "    modules = df[key_module].str.strip().tolist();\n",
    "    causes = df[key_cause].str.strip().tolist();\n",
    "    improves = df[key_improve].str.strip().tolist();\n",
    "    experiences = df[key_experience].str.strip().tolist();\n",
    "    judges = df[key_judge].str.strip().tolist();\n",
    "    scores = df[key_score].tolist(); #this is int\n",
    "    if len(questions)!= len(scores) or len(questions)!= len(severity) \\\n",
    "        or len(questions)!= len(modules) or len(questions)!= len(causes):\n",
    "        raise ValueError(\"To embedded data is not equal.\")\n",
    "    model = SentenceTransformer(encode_model);\n",
    "    # embedding question\n",
    "    question_embeddings = model.encode(\n",
    "        questions,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(f\"the encoded question shape is {question_embeddings.shape}\");\n",
    "    dimension = question_embeddings.shape[1]\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    # some the same issues:\n",
    "    # https://github.com/facebookresearch/faiss/issues/1637\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, 28)\n",
    "    index.train(question_embeddings)\n",
    "    index.add(question_embeddings)\n",
    "    faiss.write_index(index, \"../../../db/20250307v1/dqe_learn_vdb_index.faiss\")\n",
    "    #integrated with langchain\n",
    "    # 創建元數據結構\n",
    "    metadatas = [];\n",
    "    for idx in range(len(questions)):\n",
    "        tmpdict = {\n",
    "                \"severity\": severity[idx],\n",
    "                \"module\": modules[idx],\n",
    "                \"cause\":causes[idx],\n",
    "                \"improve\":improves[idx],\n",
    "                \"experience\":experiences[idx],\n",
    "                \"judge\":judges[idx],\n",
    "                \"score\":scores[idx],\n",
    "        }\n",
    "        metadatas.append(tmpdict)\n",
    "\n",
    "    # print(metadatas);\n",
    "    # 建立可持久化向量庫\n",
    "    vector_db = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(questions, question_embeddings)),\n",
    "        embedding=HuggingFaceEmbeddings(model_name=encode_model),\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    # 保存完整向量庫\n",
    "    vector_db.save_local(\"../../../db/20250307v1/dqe_learn_vdb_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e60938b-f400-4ae9-a267-1c3ee1562a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820cbbe0bf954396b2d45f4c5785db53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the encoded question shape is (28, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 28 points to 28 centroids: please provide at least 1092 training points\n"
     ]
    }
   ],
   "source": [
    "_csv_file= \"../../../doc/dsq_learn_sheet_eng_titles.csv\"\n",
    "embed_DEQ_Learn_from_csv(csv_file=_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "727396c7-d0dd-4247-9ec8-ff97f371e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_DQE(csv_file=None,qKey=\"Question\", qAns=\"Answer\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # 合併重複問題的答案\n",
    "    df_clean = df.groupby(qKey)[qAns].apply(lambda x: '\\n'.join(x.unique())).reset_index()\n",
    "    # 正規化代碼格式\n",
    "    \"\"\"\n",
    "    這是一個使用正則表達式的字串替換操作。目的是將Answer欄位中的參數設定格式標準化。\n",
    "    例如，將類似\"PcdCfgPeApmEnable = 0\"中的空格去除，變成\"PcdCfgPeApmEnable=0\"。\n",
    "    這樣做的好處是統一格式，避免後續處理時因格式不一致導致問題，比如在檢索或解析時無法正確識別參數和數值。\n",
    "    \"\"\"\n",
    "    df_clean[qAns] = df_clean[qAns].str.replace(r'(\\w+)\\s*=\\s*(\\d+)', r'\\1=\\2', regex=True)\n",
    "    # 提取文本數據\n",
    "    questions = df_clean[qKey].str.strip().tolist()\n",
    "    answers = df_clean[qAns].str.strip().tolist()\n",
    "    model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    question_embeddings = model.encode(\n",
    "        questions,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(question_embeddings.shape)\n",
    "    dimension = question_embeddings.shape[1]#(#__1)  # 向量維度(通常為384/768)\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, 100)\n",
    "    index.train(question_embeddings)\n",
    "    index.add(question_embeddings)\n",
    "    faiss.write_index(index, \"qa_index_.faiss\")\n",
    "    #integrated with langchain\n",
    "    # 創建元數據結構\n",
    "    metadatas = [{\n",
    "        \"answer\": ans, \n",
    "        \"source\": \"內部技術資料庫\",\n",
    "        \"last_updated\": \"2025-02\"\n",
    "    } for ans in answers]\n",
    "    # 建立可持久化向量庫\n",
    "    vector_db = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(questions, question_embeddings)),\n",
    "        embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"),\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    # 保存完整向量庫\n",
    "    vector_db.save_local(\"tech_support_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990a069-c0d1-4acf-9aa4-2ba9bd7cae30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
